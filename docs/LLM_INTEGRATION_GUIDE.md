# LLMé›†æˆæŒ‡å—

## æ¦‚è¿°

æœ¬ç³»ç»Ÿå·²å®ç°ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒä¸OpenAIå…¼å®¹çš„APIæœåŠ¡å™¨è¿›è¡Œäº¤äº’ï¼ŒåŒ…æ‹¬ï¼š
- **LLM**: å¤§è¯­è¨€æ¨¡å‹ï¼ˆQwen3-QwQ-32Bï¼‰
- **VLM**: è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆQwen2.5-VL-32B-Instructï¼‰
- **Embedding**: æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆbge-m3ï¼‰
- **Rerank**: æ–‡æ¡£é‡æ’åºæ¨¡å‹ï¼ˆbge-reranker-v2-minicpm-layerwiseï¼‰

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export OPENAI_API_KEY="sk-vNAqmumBY5MiaStj0fFf4eA0E88544FcB1489f7c9eB6Ed9f"
export OPENAI_API_BASE="http://192.254.90.4:3001/v1"
```

æˆ–åœ¨ä»£ç ä¸­è®¾ç½®ï¼š

```python
import os
os.environ["OPENAI_API_KEY"] = "sk-vNAqmumBY5MiaStj0fFf4eA0E88544FcB1489f7c9eB6Ed9f"
os.environ["OPENAI_API_BASE"] = "http://192.254.90.4:3001/v1"
```

### 2. åŸºç¡€ä½¿ç”¨

```python
from python-backend.agents.llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(
    llm_model="Qwen3-QwQ-32B",
    embedding_model="bge-m3"
)

# èŠå¤©è¡¥å…¨
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ä½ å¥½ï¼"}
]

response = await client.chat_completion(
    messages=messages,
    temperature=0.7,
    max_tokens=2000
)

print(response)
```

---

## æ ¸å¿ƒåŠŸèƒ½

### 1. èŠå¤©è¡¥å…¨ï¼ˆChat Completionï¼‰

```python
# åŸºç¡€èŠå¤©
response = await client.chat_completion(
    messages=[
        {"role": "system", "content": "ä½ æ˜¯æ‹›æ ‡åˆ†æä¸“å®¶ã€‚"},
        {"role": "user", "content": "è¯·åˆ†æè¿™ä»½æ‹›æ ‡æ–‡æ¡£..."}
    ],
    temperature=0.7,
    max_tokens=2000
)

# æµå¼èŠå¤©
async for chunk in client.stream_chat_completion(
    messages=messages,
    temperature=0.7,
    max_tokens=2000
):
    print(chunk, end="", flush=True)
```

### 2. è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVisionï¼‰

```python
# ä½¿ç”¨å›¾ç‰‡URL
response = await client.vision_completion(
    text="è¯·æè¿°è¿™å¼ å›¾ç‰‡",
    image_url="https://example.com/image.jpg",
    temperature=0.7,
    max_tokens=1000
)

# ä½¿ç”¨æœ¬åœ°å›¾ç‰‡
response = await client.vision_completion(
    text="è¯·åˆ†æè¿™å¼ æ‹›æ ‡æ–‡æ¡£æˆªå›¾",
    image_path="/path/to/image.jpg",
    temperature=0.7,
    max_tokens=1000
)

# ä½¿ç”¨base64ç¼–ç 
response = await client.vision_completion(
    text="è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—",
    image_base64="iVBORw0KGgoAAAANS...",
    temperature=0.7,
    max_tokens=1000
)
```

### 3. æ–‡æœ¬åµŒå…¥ï¼ˆEmbeddingï¼‰

```python
# å•ä¸ªæ–‡æœ¬
embedding = await client.create_embedding("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬")
print(f"å‘é‡ç»´åº¦: {len(embedding)}")

# æ‰¹é‡æ–‡æœ¬
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = await client.create_embedding(texts)
print(f"ç”Ÿæˆäº† {len(embeddings)} ä¸ªåµŒå…¥å‘é‡")
```

### 4. æ–‡æ¡£é‡æ’åºï¼ˆRerankï¼‰

```python
query = "æ‹›æ ‡æ–‡ä»¶åˆ†æ"
documents = [
    "æ‹›æ ‡æ–‡ä»¶åˆ†ææ˜¯æŠ•æ ‡çš„ç¬¬ä¸€æ­¥",
    "å¤©æ°”é¢„æŠ¥æ˜¾ç¤ºæ˜å¤©ä¼šä¸‹é›¨",
    "éœ€æ±‚æå–æ˜¯åˆ†æçš„æ ¸å¿ƒä»»åŠ¡"
]

results = await client.rerank(
    query=query,
    documents=documents,
    top_k=2
)

for result in results:
    print(f"[{result['score']:.4f}] {result['document']}")
```

---

## åœ¨æ™ºèƒ½ä½“ä¸­ä½¿ç”¨

### æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨LLMå®¢æˆ·ç«¯

```python
from python-backend.agents.base_agent import BaseAgent

class MyAgent(BaseAgent):
    async def _execute_impl(self, input_data):
        # ä½¿ç”¨self.llm_client
        response = await self.llm_client.chat_completion(
            messages=[
                {"role": "user", "content": input_data["query"]}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        return {"response": response}
```

### æ–¹æ³•2ï¼šä½¿ç”¨_chat_with_agentæ–¹æ³•

```python
class MyAgent(BaseAgent):
    async def _execute_impl(self, input_data):
        # ä½¿ç”¨å°è£…çš„æ–¹æ³•
        response = await self._chat_with_agent(
            message=input_data["query"],
            temperature=0.7,
            max_tokens=2000
        )
        return {"response": response}
```

---

## é…ç½®ç®¡ç†

### ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from python-backend.config.llm_config import get_llm_config

# è·å–é»˜è®¤é…ç½®
config = get_llm_config()

# è·å–ç§Ÿæˆ·ç‰¹å®šé…ç½®
config = get_llm_config("demo")

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(
    api_key=config["api_key"],
    api_base=config["api_base"],
    llm_model=config["llm_model"]
)
```

### æ›´æ–°ç§Ÿæˆ·é…ç½®

```python
from python-backend.config.llm_config import update_tenant_config

update_tenant_config("my_tenant", {
    "llm_model": "Qwen3-QwQ-32B",
    "default_temperature": 0.8,
    "default_max_tokens": 3000
})
```

---

## é«˜çº§åŠŸèƒ½

### 1. ä½¿ç”¨ç¼“å­˜

```python
from python-backend.agents.performance_optimization import with_cache, cache_manager

@with_cache(cache_manager, ttl=3600, key_prefix="analysis_")
async def analyze_document(doc: str) -> str:
    response = await client.chat_completion(
        messages=[{"role": "user", "content": f"åˆ†æ: {doc}"}],
        max_tokens=1000
    )
    return response

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šè¯·æ±‚LLM
result1 = await analyze_document("æ–‡æ¡£å†…å®¹")

# ç¬¬äºŒæ¬¡è°ƒç”¨ä¼šä»ç¼“å­˜è·å–ï¼ˆå¿«é€Ÿï¼‰
result2 = await analyze_document("æ–‡æ¡£å†…å®¹")
```

### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from python-backend.agents.error_handling import with_retry, RetryConfig

@with_retry(retry_config=RetryConfig(max_retries=3, initial_delay=1.0))
async def chat_with_retry(message: str) -> str:
    return await client.chat_completion(
        messages=[{"role": "user", "content": message}],
        max_tokens=500
    )

# è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿
response = await chat_with_retry("ä½ å¥½")
```

### 3. æ€§èƒ½ç›‘æ§

```python
from python-backend.agents.monitoring import performance_monitor

# è®°å½•LLMè°ƒç”¨
performance_monitor.record_llm_call(
    agent_name="my_agent",
    duration=1.5,
    success=True,
    token_count=1000
)

# è·å–æ€§èƒ½æ‘˜è¦
summary = performance_monitor.get_performance_summary()
print(f"LLMè°ƒç”¨æˆåŠŸç‡: {summary['llm_calls']['success_rate']:.2%}")
```

### 4. è´¨é‡æ§åˆ¶

```python
from python-backend.agents.quality_control import content_quality_checker

# æ£€æŸ¥ç”Ÿæˆå†…å®¹çš„è´¨é‡
response = await client.chat_completion(messages=messages)

quality_checks = content_quality_checker.comprehensive_check(
    response,
    required_sections=["introduction", "analysis", "conclusion"]
)

if quality_checks["overall_passed"]:
    print("âœ… è´¨é‡æ£€æŸ¥é€šè¿‡")
else:
    print(f"âŒ å‘ç°é—®é¢˜: {quality_checks['issues']}")
```

---

## æµ‹è¯•

### è¿è¡ŒLLMå®¢æˆ·ç«¯æµ‹è¯•

```bash
# æµ‹è¯•æ‰€æœ‰åŠŸèƒ½
python python-backend/tests/test_llm_client.py

# è¿è¡Œç¤ºä¾‹
python python-backend/examples/llm_usage_example.py
```

### æµ‹è¯•è¾“å‡ºç¤ºä¾‹

```
=== æµ‹è¯•LLMå®¢æˆ·ç«¯è¿æ¥ ===

âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ
   API Base: http://192.254.90.4:3001/v1
   LLM Model: Qwen3-QwQ-32B
   VLM Model: Qwen2.5-VL-32B-Instruct
   Embedding Model: bge-m3
   Rerank Model: bge-reranker-v2-minicpm-layerwise

=== æµ‹è¯•èŠå¤©è¡¥å…¨ ===

å‘é€è¯·æ±‚...
âœ… èŠå¤©è¡¥å…¨æˆåŠŸ
   å“åº”é•¿åº¦: 245 å­—ç¬¦
   å“åº”å†…å®¹:
æ‹›æ ‡æ–‡ä»¶åˆ†æçš„ä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š
1. æ–‡æ¡£åˆ†ç±»å’Œåˆæ­¥å®¡æŸ¥
2. éœ€æ±‚æå–å’Œæ•´ç†
3. æŠ€æœ¯è§„èŒƒåˆ†æ
4. å•†åŠ¡æ¡æ¬¾å®¡æŸ¥
5. é£é™©è¯„ä¼°å’Œå¯è¡Œæ€§åˆ†æ...

=== æµ‹è¯•æ–‡æœ¬åµŒå…¥ ===

å‘é€è¯·æ±‚...
âœ… åµŒå…¥ç”ŸæˆæˆåŠŸ
   å‘é‡ç»´åº¦: 1024
   å‰10ä¸ªå€¼: [0.123, -0.456, 0.789, ...]

=== æµ‹è¯•æ–‡æ¡£é‡æ’åº ===

å‘é€è¯·æ±‚...
âœ… é‡æ’åºæˆåŠŸ
   åŸå§‹æ–‡æ¡£æ•°: 5
   è¿”å›ç»“æœæ•°: 3

   æ’åºç»“æœ:
   1. [åˆ†æ•°: 0.8523] æ‹›æ ‡æ–‡ä»¶åˆ†ææ˜¯æŠ•æ ‡è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥
   2. [åˆ†æ•°: 0.7891] éœ€æ±‚æå–æ˜¯æ‹›æ ‡åˆ†æçš„æ ¸å¿ƒä»»åŠ¡
   3. [åˆ†æ•°: 0.6234] é£é™©è¯„ä¼°å¸®åŠ©è¯†åˆ«æ½œåœ¨é—®é¢˜

æµ‹è¯•æ€»ç»“
   èŠå¤©è¡¥å…¨: âœ… é€šè¿‡
   æ–‡æœ¬åµŒå…¥: âœ… é€šè¿‡
   æ‰¹é‡åµŒå…¥: âœ… é€šè¿‡
   æ–‡æ¡£é‡æ’åº: âœ… é€šè¿‡
   æµå¼è¡¥å…¨: âœ… é€šè¿‡

   æ€»è®¡: 5/5 æµ‹è¯•é€šè¿‡
   æˆåŠŸç‡: 100.0%

ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMå®¢æˆ·ç«¯å·¥ä½œæ­£å¸¸ã€‚
```

---

## æ¨¡å‹é…ç½®

### å¯ç”¨æ¨¡å‹

| ç±»å‹ | æ¨¡å‹åç§° | ç”¨é€” | ä¸Šä¸‹æ–‡çª—å£ |
|------|----------|------|------------|
| LLM | Qwen3-QwQ-32B | æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ | 32K |
| VLM | Qwen2.5-VL-32B-Instruct | å›¾åƒç†è§£ | 32K |
| Embedding | bge-m3 | æ–‡æœ¬åµŒå…¥ | 8K |
| Rerank | bge-reranker-v2-minicpm-layerwise | æ–‡æ¡£é‡æ’åº | - |

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| temperature | float | 0.7 | æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ |
| max_tokens | int | 2000 | æœ€å¤§ç”Ÿæˆtokenæ•° |
| top_p | float | 0.9 | æ ¸é‡‡æ ·å‚æ•° |
| stream | bool | False | æ˜¯å¦æµå¼è¾“å‡º |

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šè¿æ¥å¤±è´¥

**ç—‡çŠ¶**: `Connection refused` æˆ– `Timeout`

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦è¿è¡Œï¼š`curl http://192.254.90.4:3001/v1/models`
2. æ£€æŸ¥ç½‘ç»œè¿æ¥
3. éªŒè¯API_BASE URLæ˜¯å¦æ­£ç¡®

### é—®é¢˜2ï¼šè®¤è¯å¤±è´¥

**ç—‡çŠ¶**: `401 Unauthorized`

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥API_KEYæ˜¯å¦æ­£ç¡®
2. éªŒè¯ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®ï¼š`echo $OPENAI_API_KEY`

### é—®é¢˜3ï¼šæ¨¡å‹ä¸å¯ç”¨

**ç—‡çŠ¶**: `Model not found`

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
2. æŸ¥çœ‹å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š`curl http://192.254.90.4:3001/v1/models`

### é—®é¢˜4ï¼šå“åº”æ…¢

**ç—‡çŠ¶**: è¯·æ±‚æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
1. å¯ç”¨ç¼“å­˜
2. å‡å°‘max_tokens
3. ä½¿ç”¨æµå¼è¾“å‡º
4. æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ

---

## æœ€ä½³å®è·µ

### 1. ä½¿ç”¨åˆé€‚çš„æ¸©åº¦å‚æ•°

```python
# éœ€è¦åˆ›é€ æ€§çš„ä»»åŠ¡ï¼ˆå¦‚å†…å®¹ç”Ÿæˆï¼‰
temperature = 0.8

# éœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡ï¼ˆå¦‚æ•°æ®æå–ï¼‰
temperature = 0.3

# å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
temperature = 0.7  # é»˜è®¤å€¼
```

### 2. æ§åˆ¶tokenä½¿ç”¨

```python
# çŸ­å›ç­”
max_tokens = 500

# ä¸­ç­‰é•¿åº¦
max_tokens = 2000  # é»˜è®¤å€¼

# é•¿æ–‡æ¡£
max_tokens = 4000
```

### 3. ä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯

```python
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æ ‡åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰15å¹´ç»éªŒã€‚"
    },
    {
        "role": "user",
        "content": "è¯·åˆ†æè¿™ä»½æ‹›æ ‡æ–‡æ¡£..."
    }
]
```

### 4. å¯ç”¨ç¼“å­˜

å¯¹äºé‡å¤çš„æŸ¥è¯¢ï¼Œä½¿ç”¨ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½ï¼š

```python
@with_cache(cache_manager, ttl=3600)
async def cached_analysis(doc: str):
    return await client.chat_completion(...)
```

### 5. é”™è¯¯å¤„ç†

å§‹ç»ˆä½¿ç”¨try-exceptå’Œé‡è¯•æœºåˆ¶ï¼š

```python
@with_retry(retry_config=RetryConfig(max_retries=3))
async def robust_chat(message: str):
    try:
        return await client.chat_completion(...)
    except Exception as e:
        logger.error(f"Chat failed: {e}")
        raise
```

---

## APIå‚è€ƒ

### LLMClientç±»

```python
class LLMClient:
    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        llm_model: str = "Qwen3-QwQ-32B",
        vlm_model: str = "Qwen2.5-VL-32B-Instruct",
        embedding_model: str = "bge-m3",
        rerank_model: str = "bge-reranker-v2-minicpm-layerwise"
    )
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        model: Optional[str] = None,
        stream: bool = False,
        **kwargs
    ) -> str
    
    async def vision_completion(
        self,
        text: str,
        image_path: Optional[str] = None,
        image_url: Optional[str] = None,
        image_base64: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str
    
    async def create_embedding(
        self,
        text: Union[str, List[str]],
        model: Optional[str] = None
    ) -> Union[List[float], List[List[float]]]
    
    async def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: Optional[int] = None,
        model: Optional[str] = None
    ) -> List[Dict[str, Any]]
    
    async def stream_chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000,
        model: Optional[str] = None,
        **kwargs
    )
```

---

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2024-11-10)
- âœ… å®ç°ç»Ÿä¸€LLMå®¢æˆ·ç«¯
- âœ… æ”¯æŒèŠå¤©è¡¥å…¨ã€è§†è§‰ã€åµŒå…¥ã€é‡æ’åº
- âœ… é›†æˆåˆ°æ™ºèƒ½ä½“ç³»ç»Ÿ
- âœ… æ·»åŠ ç¼“å­˜å’Œé”™è¯¯å¤„ç†
- âœ… å®Œæ•´çš„æµ‹è¯•å’Œæ–‡æ¡£

---

## è”ç³»æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- ğŸ“– [ä½¿ç”¨æŒ‡å—](./AGENT_SYSTEM_USAGE_GUIDE.md)
- ğŸ§ª [æµ‹è¯•æ–‡ä»¶](./python-backend/tests/test_llm_client.py)
- ğŸ’¡ [ç¤ºä¾‹ä»£ç ](./python-backend/examples/llm_usage_example.py)

---

**æœ€åæ›´æ–°**: 2024-11-10  
**ç‰ˆæœ¬**: 1.0.0  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
