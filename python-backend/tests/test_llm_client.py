"""
LLMå®¢æˆ·ç«¯æµ‹è¯•
æµ‹è¯•ä¸OpenAIå…¼å®¹æœåŠ¡å™¨çš„è¿æ¥å’ŒåŠŸèƒ½
"""
import asyncio
import os
from python-backend.agents.llm_client import LLMClient


async def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥"""
    print("=== æµ‹è¯•LLMå®¢æˆ·ç«¯è¿æ¥ ===\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦å·²è®¾ç½®
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  è­¦å‘Š: OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åˆ›å»º .env æ–‡ä»¶")
        return None
    
    if not os.getenv("OPENAI_API_BASE"):
        print("âš ï¸  è­¦å‘Š: OPENAI_API_BASE ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åˆ›å»º .env æ–‡ä»¶")
        return None
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = LLMClient(
        llm_model="Qwen3-QwQ-32B",
        vlm_model="Qwen2.5-VL-32B-Instruct",
        embedding_model="bge-m3",
        rerank_model="bge-reranker-v2-minicpm-layerwise"
    )
    
    print(f"âœ… å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    print(f"   API Base: {client.api_base}")
    print(f"   LLM Model: {client.llm_model}")
    print(f"   VLM Model: {client.vlm_model}")
    print(f"   Embedding Model: {client.embedding_model}")
    print(f"   Rerank Model: {client.rerank_model}\n")
    
    return client


async def test_chat_completion(client: LLMClient):
    """æµ‹è¯•èŠå¤©è¡¥å…¨"""
    print("=== æµ‹è¯•èŠå¤©è¡¥å…¨ ===\n")
    
    try:
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æ ‡æ–‡ä»¶åˆ†æä¸“å®¶ã€‚"},
            {"role": "user", "content": "è¯·ç®€è¦è¯´æ˜æ‹›æ ‡æ–‡ä»¶åˆ†æçš„ä¸»è¦æ­¥éª¤ã€‚"}
        ]
        
        print("å‘é€è¯·æ±‚...")
        response = await client.chat_completion(
            messages=messages,
            temperature=0.7,
            max_tokens=500
        )
        
        print(f"âœ… èŠå¤©è¡¥å…¨æˆåŠŸ")
        print(f"   å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"   å“åº”å†…å®¹:\n{response[:200]}...\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ èŠå¤©è¡¥å…¨å¤±è´¥: {str(e)}\n")
        return False


async def test_embedding(client: LLMClient):
    """æµ‹è¯•æ–‡æœ¬åµŒå…¥"""
    print("=== æµ‹è¯•æ–‡æœ¬åµŒå…¥ ===\n")
    
    try:
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºç”ŸæˆåµŒå…¥å‘é‡ã€‚"
        
        print("å‘é€è¯·æ±‚...")
        embedding = await client.create_embedding(text)
        
        print(f"âœ… åµŒå…¥ç”ŸæˆæˆåŠŸ")
        print(f"   å‘é‡ç»´åº¦: {len(embedding)}")
        print(f"   å‰10ä¸ªå€¼: {embedding[:10]}\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}\n")
        return False


async def test_batch_embedding(client: LLMClient):
    """æµ‹è¯•æ‰¹é‡åµŒå…¥"""
    print("=== æµ‹è¯•æ‰¹é‡åµŒå…¥ ===\n")
    
    try:
        texts = [
            "ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            "ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æœ¬",
            "ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æœ¬"
        ]
        
        print("å‘é€è¯·æ±‚...")
        embeddings = await client.create_embedding(texts)
        
        print(f"âœ… æ‰¹é‡åµŒå…¥æˆåŠŸ")
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   åµŒå…¥æ•°é‡: {len(embeddings)}")
        print(f"   å‘é‡ç»´åº¦: {len(embeddings[0])}\n")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡åµŒå…¥å¤±è´¥: {str(e)}\n")
        return False


async def test_rerank(client: LLMClient):
    """æµ‹è¯•æ–‡æ¡£é‡æ’åº"""
    print("=== æµ‹è¯•æ–‡æ¡£é‡æ’åº ===\n")
    
    try:
        query = "æ‹›æ ‡æ–‡ä»¶åˆ†æ"
        documents = [
            "æ‹›æ ‡æ–‡ä»¶åˆ†ææ˜¯æŠ•æ ‡è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥",
            "å¤©æ°”é¢„æŠ¥æ˜¾ç¤ºæ˜å¤©ä¼šä¸‹é›¨",
            "éœ€æ±‚æå–æ˜¯æ‹›æ ‡åˆ†æçš„æ ¸å¿ƒä»»åŠ¡",
            "ä»Šå¤©çš„åˆé¤å¾ˆç¾å‘³",
            "é£é™©è¯„ä¼°å¸®åŠ©è¯†åˆ«æ½œåœ¨é—®é¢˜"
        ]
        
        print("å‘é€è¯·æ±‚...")
        results = await client.rerank(
            query=query,
            documents=documents,
            top_k=3
        )
        
        print(f"âœ… é‡æ’åºæˆåŠŸ")
        print(f"   åŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
        print(f"   è¿”å›ç»“æœæ•°: {len(results)}")
        print(f"\n   æ’åºç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"   {i}. [åˆ†æ•°: {result['score']:.4f}] {result['document']}")
        print()
        
        return True
        
    except Exception as e:
        print(f"âŒ é‡æ’åºå¤±è´¥: {str(e)}\n")
        return False


async def test_stream_completion(client: LLMClient):
    """æµ‹è¯•æµå¼è¡¥å…¨"""
    print("=== æµ‹è¯•æµå¼è¡¥å…¨ ===\n")
    
    try:
        messages = [
            {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"}
        ]
        
        print("å‘é€æµå¼è¯·æ±‚...")
        print("å“åº”: ", end="", flush=True)
        
        full_response = ""
        async for chunk in client.stream_chat_completion(
            messages=messages,
            temperature=0.7,
            max_tokens=100
        ):
            print(chunk, end="", flush=True)
            full_response += chunk
        
        print(f"\n\nâœ… æµå¼è¡¥å…¨æˆåŠŸ")
        print(f"   æ€»é•¿åº¦: {len(full_response)} å­—ç¬¦\n")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµå¼è¡¥å…¨å¤±è´¥: {str(e)}\n")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("LLMå®¢æˆ·ç«¯åŠŸèƒ½æµ‹è¯•")
    print("="*60 + "\n")
    
    try:
        # æµ‹è¯•è¿æ¥
        client = await test_llm_connection()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        results = {
            "èŠå¤©è¡¥å…¨": await test_chat_completion(client),
            "æ–‡æœ¬åµŒå…¥": await test_embedding(client),
            "æ‰¹é‡åµŒå…¥": await test_batch_embedding(client),
            "æ–‡æ¡£é‡æ’åº": await test_rerank(client),
            "æµå¼è¡¥å…¨": await test_stream_completion(client)
        }
        
        # æ‰“å°æµ‹è¯•æ€»ç»“
        print("="*60)
        print("æµ‹è¯•æ€»ç»“")
        print("="*60 + "\n")
        
        passed = sum(1 for v in results.values() if v)
        total = len(results)
        
        for test_name, result in results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"   {test_name}: {status}")
        
        print(f"\n   æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
        print(f"   æˆåŠŸç‡: {passed/total*100:.1f}%\n")
        
        if passed == total:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMå®¢æˆ·ç«¯å·¥ä½œæ­£å¸¸ã€‚\n")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {str(e)}\n")


if __name__ == "__main__":
    asyncio.run(main())
